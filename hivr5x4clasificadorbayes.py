# -*- coding: utf-8 -*-
"""HIVR5X4ClasificadorBayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SmzCjA8SpolKyl9Yx3m9dEdqln0UbjCY
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors
import seaborn as sb
import glob
import keras as k


# %matplotlib inline
plt.rcParams['figure.figsize'] = (16, 9)
plt.style.use('ggplot')

from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.feature_selection import SelectKBest
from keras.layers import Dense
from keras.models import Sequential, load_model

#Importante ejectuar esto para poder hacer uso de tf en colab
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

from google.colab import drive
drive.mount('/content/drive')

#Importamos nuestro csv
df = pd.read_csv(r"/content/drive/My Drive/Netttt/renal.csv")
df.head(25)
df.describe()

df2 = df.copy()
#remplazamos los valores perdidos con los que mas se repiten, esto lo vemos gracias al codigo anterior
rbcNormal = df2['rbc'].str.replace('?', 'normal')
df2['rbc'] = rbcNormal
pcNormal = df2['pc'].str.replace('?', 'normal')
df2['pc'] = pcNormal
pccNotpresent = df2['pcc'].str.replace('?', 'notpresent')
df2['pcc'] = pccNotpresent
BaNotpresent = df2['ba'].str.replace('?', 'notpresent')
df2['ba'] = BaNotpresent
htnno = df2 ['htn'].str.replace('?', 'no')
df2['htn'] = htnno
dmno = df2 ['dm'].str.replace('?', 'no')
df2['dm'] = dmno
cadno = df2 ['cad'].str.replace('?', 'no')
df2['cad'] = htnno
appetGood = df2['appet'].str.replace('?', 'good')
df2['appet'] = appetGood
peNo = df2['pe'].str.replace('?', 'no')
df2['pe'] = peNo
aneNo = df2['ane'].str.replace('?', 'no')
df2['ane'] = aneNo
#Asignamos 0 y 1 a nuestra class para entrenar el modelo.
classckd = df['class'].str.replace('ckd', '0')
df['class'] = classckd
classnotckd = df['class'].str.replace('not0', '1')
df['class'] = classnotckd

#Eliminamos las columnas con datos categoricos
df.drop(['rbc'],1, inplace=True)
df.drop(['pc'],1, inplace=True)
df.drop(['pcc'],1, inplace=True)
df.drop(['ba'],1, inplace=True)
df.drop(['htn'],1, inplace=True)
df.drop(['dm'],1, inplace=True)
df.drop(['cad'],1, inplace=True)
df.drop(['appet'],1, inplace=True)
df.drop(['pe'],1, inplace=True)
df.drop(['ane'],1, inplace=True)
X = df.drop(['class'], axis=1, inplace=False)
Y = df.loc[:,['class']]
X.replace('?', np.nan, inplace=True)
columnNames = list(X.columns.values)
#Eliminamos los nomninales.
for names in columnNames:
    df2.drop([names],1, inplace=True)
df2.drop(['class'], axis=1, inplace=True)
#Llenamos los datos perdidos con el promedio de su columna, a esto se le llama dato centrado, al igual que con los categoricos.
for names in columnNames:
    testcolumn = np.array(X[names]).astype(float)
    mean = np.nanmean(testcolumn.astype(float))
    testcolumn[np.isnan(testcolumn)]=mean
    X[names] = testcolumn

#Juntamos nuestras dos columnas
df_col = pd.concat([df2,X], axis=1)
X = df_col
X.to_csv("X.csv",index=False)
finalcsv = pd.concat([X,Y], axis=1)
finalcsv.to_csv("final.csv",index=False)

def load(filepath):
    raw_data = pd.read_csv(filepath)
    processed_columns = pd.DataFrame({})
    for col in raw_data:
        if raw_data[col].dtype == 'object' or raw_data[col].nunique() < 7:
            df = pd.get_dummies(raw_data[col], prefix=col)
            processed_columns = pd.concat([processed_columns, df], axis=1)
        else:
            processed_columns = pd.concat([processed_columns, raw_data[col]], axis=1)
    return processed_columns
X = load("X.csv")
df2 = pd.read_csv("final.csv")
Y = df2.loc[:,['class']]

df2.head(50)

#ESTO es para descargar el nuevo dataset y despues importarlo para hacer uso de el, ya que si no  genera problemas.
print(df2.groupby('class').size())
#from google.colab import files
#files.download("data.csv")
df3 = pd.read_csv(r"/content/drive/My Drive/Netttt/final2.csv")

#Generamos histograma de los datos numericos
df3.drop(['class','rbc','pc','pcc','ba','htn','dm','cad','appet','pe','ane'], axis=1).hist()
plt.show()

#Sacamos estadistica descriptiva de los datos numericos.
reduced = df3.drop(['class','rbc','pc','pcc','ba','htn','dm','cad','appet','pe','ane'], axis=1)
reduced.describe()

#Hacemos un mapa de colores donde se vea la relaciòn entre cada columna
colormap = plt.cm.viridis
plt.figure(figsize=(12,12))
plt.title('Pearson Correlation of Features', y=1.05, size=15)
sb.heatmap(reduced.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)

#Graficas de relaciòn entre cada columna (no ejecutamos, tarda mucho.)
sb.pairplot(df3, hue='class',size=4,kind='scatter')

#Seleccionamos nuestro csv y eliminamos los datos categoricos para entrenar nuestro modelo.
df3 = pd.read_csv(r"/content/drive/My Drive/Netttt/final2.csv")
X = df3.drop(['class','rbc','pc','pcc','ba','htn','dm','cad','appet','pe','ane'], axis=1)
y = df3["class"]
X_train, X_test = train_test_split(df2, test_size=0.95, random_state=100000) 
y_train =X_train["class"]
y_test = X_test["class"]

x_scaler = MinMaxScaler()
x_scaler.fit(X)
column_names = X.columns
X[column_names] = x_scaler.transform(X)

#Asignamos el tamaño de nuestro datatrain y datatest
X_train,  X_test, y_train, y_test = train_test_split(
        X, y, test_size= 0.2, shuffle=True)

#Importamos el modelo.
model = Sequential()
model.add(Dense(256, input_dim=len(X.columns),
                    kernel_initializer=k.initializers.random_normal(seed=13), activation="relu"))
model.add(Dense(1, activation="hard_sigmoid"))

model.compile(loss='binary_crossentropy', 
                  optimizer='adam', metrics=['accuracy'])

#Entrenamos
history = model.fit(X_train, y_train, 
                    epochs=2000, #The number of iterations over the entire dataset to train on
                    batch_size=X_train.shape[0])

#guardamos nuestro modelo
model.save("ckd.model")

#graficamos nuestros resultados.
plt.plot(history.history["acc"])
plt.plot(history.history["loss"])
plt.title("model accuracy & loss")
plt.ylabel("accuracy and loss")
plt.xlabel("epoch")
plt.legend(['acc', 'loss'], loc='lower right')
plt.show()

#imprimos valores del entrenamiento y test
print("-------------------------------------------------------------------")
print("Shape of training data: ", X_train.shape)
print("Shape of test data    : ", X_test.shape )
print("-------------------------------------------------------------------")

#Vemos una predicion y la evaluamos.
for model_file in glob.glob("*.model"):
  print("Model file: ", model_file)
  model = load_model(model_file)
  pred = model.predict(X_test)
  pred = [1 if y>=0.5 else 0 for y in pred] #Threshold, transforming probabilities to either 0 or 1 depending if the probability is below or above 0.5
  scores = model.evaluate(X_test, y_test)
  print()
  print("Original  : {0}".format(", ".join([str(x) for x in y_test])))
  print()
  print("Predicted : {0}".format(", ".join([str(x) for x in pred])))
  print() 
  print("Scores    : loss = ", scores[0], " acc = ", scores[1])
  print("-------------------------------------------------------------------")

#Naive Bayes, aquì seleccionamos los 5 mejores de todos nuestros datos, omitiendo los valores categoricos
X=df3.drop(['class','rbc','pc','pcc','ba','htn','dm','cad','appet','pe','ane'], axis=1)
y=df3['class']
 
best=SelectKBest(k=5)
X_new = best.fit_transform(X, y)
X_new.shape
selected = best.get_support(indices=True)
print(X.columns[selected])

·#Al igual que antes vemos la relaciòn entre cada valor de los 5 seleccionados,
used_features =X.columns[selected]
colormap = plt.cm.viridis
plt.figure(figsize=(12,12))
plt.title('Pearson Correlation of Features', y=1.05, size=15)
sb.heatmap(df3[used_features].astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)

X_train, X_test = train_test_split(df3, test_size=0.2, random_state=6) 
y_train =X_train["class"]
y_test = X_test["class"]

# llamamdos al clasificador de bayes
gnb = GaussianNB()
# entrenamos 
gnb.fit(
    X_train[used_features].values,
    y_train
)
y_pred = gnb.predict(X_test[used_features])
 
print('Precisión en el set de Entrenamiento: {:.2f}'
     .format(gnb.score(X_train[used_features], y_train)))
print('Precisión en el set de Test: {:.2f}'
     .format(gnb.score(X_test[used_features], y_test)))

#Vemos que tuvimos mayor exito con Naive Bayes por lo que usaremos este para predecir.
#                 ['sg', 'al', 'hemo', 'pcv', 'rc']
print(gnb.predict([[0,        0,     0,       0,         0],
                   [0,        0,    0,       0,       0] ]))
#0-CKD, 1-NotCKD
